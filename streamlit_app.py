#!/usr/bin/env python3
"""
Streamlitå˜èªå¸³ã‚¢ãƒ—ãƒª - ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æœªç¿’å¾—å˜èªè¡¨ç¤º
"""

import os
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from notion_client import Client

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


@st.cache_resource
def get_notion_client():
    """Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆãƒªã‚½ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰"""
    notion_token = os.getenv("NOTION_TOKEN")
    if not notion_token:
        st.error("NOTION_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        st.stop()
    return Client(auth=notion_token)


@st.cache_data
def get_sentences_data():
    """Sentencesãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    notion = get_notion_client()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
    sentences_db_id = "2230dc53-a13b-8055-9c36-cbe6162846ef"

    try:
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        all_results = []
        has_more = True
        start_cursor = None

        while has_more:
            query_params = {
                "database_id": sentences_db_id,
                "page_size": 100
            }
            if start_cursor:
                query_params["start_cursor"] = start_cursor

            result = notion.databases.query(**query_params)
            all_results.extend(result['results'])
            has_more = result['has_more']
            start_cursor = result.get('next_cursor')

        # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
        sentences_data = []
        for page in all_results:
            sentence_text = ""
            unmastered_words = ""
            section = None
            no = None

            for prop_name, prop_value in page['properties'].items():
                prop_type = prop_value.get('type')

                if prop_type == 'title':
                    # ä¾‹æ–‡
                    title_content = prop_value.get('title')
                    if title_content and len(title_content) > 0:
                        sentence_text = title_content[0]['plain_text']

                elif prop_type == 'formula':
                    # Unmastered Words
                    formula_result = prop_value.get('formula', {})
                    if formula_result.get('type') == 'string':
                        string_value = formula_result.get('string')
                        if string_value and string_value.strip():
                            if 'unmastered' in prop_name.lower():
                                unmastered_words = string_value

                elif prop_type == 'number':
                    # Section, No
                    number_value = prop_value.get('number')
                    if number_value is not None:
                        if prop_name.lower() == 'section':
                            section = int(number_value)
                        elif prop_name.lower() == 'no':
                            no = int(number_value)

            if sentence_text and unmastered_words:
                sentences_data.append({
                    'section': section,
                    'no': no,
                    'sentence': sentence_text,
                    'unmastered_words': unmastered_words,
                    'page_id': page['id']
                })

        return sentences_data

    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    st.set_page_config(
        page_title="Wordbook - ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æœªç¿’å¾—å˜èª",
        page_icon="ğŸ“š",
        layout="wide"
    )

    st.title("ğŸ“š Wordbook")

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        sentences_data = get_sentences_data()

    if not sentences_data:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # DataFrameã«å¤‰æ›
    df = pd.DataFrame(sentences_data)

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
    st.sidebar.header("ğŸ” Filter")

    # æœªç¿’å¾—å˜èªã§ã®ãƒ•ã‚£ãƒ«ã‚¿
    search_word = st.sidebar.text_input(
        "Search",
        placeholder="ä¾‹: density, gradually",
        help="éƒ¨åˆ†ä¸€è‡´ã§æ¤œç´¢ã—ã¾ã™"
    )

    # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
    filtered_df = df.copy()

    if search_word:
        filtered_df = filtered_df[
            filtered_df['unmastered_words'].str.contains(
                search_word, case=False, na=False
            )
        ]

    # ä¾‹æ–‡ä¸€è¦§è¡¨ç¤º
    if not filtered_df.empty:
        st.header("ğŸ“– Sentences contains unmastered words.")
        st.markdown(f"**{len(filtered_df)}** sentences found")

        # è¡¨å½¢å¼ã§è¡¨ç¤º
        columns = ['section', 'no', 'sentence', 'unmastered_words']
        display_df = filtered_df[columns].copy()
        display_df = display_df.sort_values(['section', 'no'])
        display_df.columns = ['Section', 'No.', 'ä¾‹æ–‡', 'æœªç¿’å¾—å˜èª']

        st.dataframe(
            display_df,
            use_container_width=True,
            hide_index=True,
            column_config={
                "Section": st.column_config.NumberColumn(
                    width="small"
                ),
                "No.": st.column_config.NumberColumn(
                    width="small"
                ),
                "ä¾‹æ–‡": st.column_config.TextColumn(
                    width="large"
                ),
                "æœªç¿’å¾—å˜èª": st.column_config.TextColumn(
                    width="medium"
                )
            }
        )
    else:
        st.info("æœªç¿’å¾—å˜èªãŒã‚ã‚‹ä¾‹æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


if __name__ == "__main__":
    main()
