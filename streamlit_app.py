#!/usr/bin/env python3
"""
Streamlitå˜èªå¸³ã‚¢ãƒ—ãƒª - ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æœªç¿’å¾—å˜èªè¡¨ç¤º
"""

import os
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from notion_client import Client

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


@st.cache_resource
def get_notion_client():
    """Notionã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆãƒªã‚½ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰"""
    notion_token = os.getenv("NOTION_TOKEN")
    if not notion_token:
        st.error("NOTION_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        st.stop()
    return Client(auth=notion_token)


@st.cache_data(ttl=60, show_spinner=False)  # ã‚¹ãƒ”ãƒŠãƒ¼ã‚’éè¡¨ç¤º
def get_sentence_texts(sentence_ids):
    """ä¾‹æ–‡IDãƒªã‚¹ãƒˆã‹ã‚‰ä¾‹æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬å–å¾—"""
    if not sentence_ids:
        return []

    notion = get_notion_client()
    sentences = []

    # ãƒãƒƒãƒå‡¦ç†ã§ä¾‹æ–‡ã‚’å–å¾—ï¼ˆæœ€å¤§10å€‹ã¾ã§ï¼‰
    for sentence_id in sentence_ids[:10]:  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®ãŸã‚æœ€å¤§10å€‹ã«åˆ¶é™
        try:
            sentence_page = notion.pages.retrieve(page_id=sentence_id)
            properties = sentence_page['properties']
            title_prop = properties.get('ä¾‹æ–‡')
            if title_prop and title_prop.get('type') == 'title':
                titles = title_prop.get('title', [])
                if titles and len(titles) > 0:
                    sentence_parts = []
                    for text_element in titles:
                        if text_element.get('plain_text'):
                            sentence_parts.append(text_element['plain_text'])
                    text = ''.join(sentence_parts).strip()
                    sentences.append(text)
        except Exception:
            continue

    return sentences


def get_status_emoji(status):
    """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¯¾å¿œã™ã‚‹emojiã‚’è¿”ã™"""
    status_map = {
        'Not Sure': 'ğŸ¤”',
        'Seen It': 'ğŸ‘€',
        'Almost There': 'ğŸ˜ƒ',
        'Mastered': 'âœ…',  # é€šå¸¸ã¯è¡¨ç¤ºã•ã‚Œãªã„ãŒå¿µã®ãŸã‚
        None: 'â“',
        '': 'â“'
    }
    return status_map.get(status, 'â“')


# @st.cache_data(ttl=60, show_spinner=False)  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
def get_words_data():
    """Wordsãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    notion = get_notion_client()  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
    words_db_id = "2230dc53-a13b-8007-91d2-c3ed98f8dc95"  # Wordsãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ID

    try:
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        all_results = []
        has_more = True
        start_cursor = None

        while has_more:
            query_params = {
                "database_id": words_db_id,
                "page_size": 100
            }
            if start_cursor:
                query_params["start_cursor"] = start_cursor

            result = notion.databases.query(**query_params)
            all_results.extend(result['results'])
            has_more = result['has_more']
            start_cursor = result.get('next_cursor')

        # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
        words_data = []
        for page in all_results:
            word_text = ""
            section = None
            no = None
            status = None
            sentences = []

            for prop_name, prop_value in page['properties'].items():
                prop_type = prop_value.get('type')

                if prop_type == 'title':
                    # å˜èª
                    title_content = prop_value.get('title')
                    if title_content and len(title_content) > 0:
                        # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
                        word_parts = []
                        for text_element in title_content:
                            if text_element.get('plain_text'):
                                word_parts.append(text_element['plain_text'])
                        word_text = ''.join(word_parts).strip()

                elif prop_type == 'relation':
                    # Sentences (relation) - IDã®ã¿åé›†ã€å¾Œã§ãƒãƒƒãƒå–å¾—
                    if 'sentences' in prop_name.lower():
                        relation_data = prop_value.get('relation', [])
                        sentence_ids = []
                        for relation_item in relation_data:
                            sentence_id = relation_item.get('id')
                            if sentence_id:
                                sentence_ids.append(sentence_id)
                        sentences = sentence_ids  # IDãƒªã‚¹ãƒˆã‚’ä¿å­˜

                elif prop_type == 'rollup':
                    # Section, No (rollup)
                    rollup_result = prop_value.get('rollup', {})

                    if rollup_result.get('type') == 'array':
                        # rollupãŒé…åˆ—ã®å ´åˆ
                        array_data = rollup_result.get('array', [])
                        if array_data and len(array_data) > 0:
                            first_item = array_data[0]
                            if first_item.get('type') == 'number':
                                number_value = first_item.get('number')
                                if number_value is not None:
                                    if prop_name.lower() == 'section':
                                        section = int(number_value)
                                    elif 'no' in prop_name.lower():
                                        no = int(number_value)
                    elif rollup_result.get('type') == 'number':
                        # rollupãŒç›´æ¥æ•°å€¤ã®å ´åˆ
                        number_value = rollup_result.get('number')
                        if number_value is not None:
                            if prop_name.lower() == 'section':
                                section = int(number_value)
                            elif 'no' in prop_name.lower():
                                no = int(number_value)

                elif prop_type == 'status':
                    # Status
                    if 'status' in prop_name.lower():
                        status_obj = prop_value.get('status')
                        if status_obj:
                            status = status_obj.get('name', '')

            # æœªç¿’å¾—ã®å˜èªã®ã¿ã‚’å–å¾— (StatusãŒ"Mastered"ã§ãªã„ã‚‚ã®)
            if word_text and status != "Mastered":
                words_data.append({
                    'Section': section,
                    'No.': no,
                    'Word': word_text,
                    'Status': status,
                    'sentence_ids': sentences,  # ä¾‹æ–‡IDã‚’ä¿å­˜
                    'page_id': page['id']
                })

        return words_data

    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    st.set_page_config(
        page_title="Wordbook - ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æœªç¿’å¾—å˜èª",
        page_icon="ğŸ“š",
        layout="wide"
    )

    st.title("ğŸ“š Wordbook")

    # Notionæ¥ç¶šãƒ†ã‚¹ãƒˆ
    try:
        get_notion_client()
    except Exception as e:
        st.error(f"Notion APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    with st.spinner("Loading unmastered words..."):
        words_data = get_words_data()

    if not words_data:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # DataFrameã«å¤‰æ›
    df = pd.DataFrame(words_data)

    # å˜èªé¸æŠã¨ä¾‹æ–‡è¡¨ç¤º
    if not df.empty:
        st.header("ğŸ“– Unmastered words.")
        st.markdown(f"**{len(df)}** words found")

        # ã‚½ãƒ¼ãƒˆæ¸ˆã¿ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        sorted_df = df.sort_values(['Section', 'No.'])

        # å˜èªé¸æŠ
        word_options = []
        for _, row in sorted_df.iterrows():
            section = row['Section'] if row['Section'] is not None else '?'
            no = row['No.'] if row['No.'] is not None else '?'
            word = row['Word']
            status = row['Status'] if row['Status'] else 'Unknown'
            status_emoji = get_status_emoji(status)
            display_text = f"Section {section}-{no}: {status_emoji} {word}"
            word_options.append(display_text)

        selected_display = st.selectbox(
            "Select a word:",
            options=word_options,
            help="å˜èªã‚’é¸æŠã™ã‚‹ã¨ä¾‹æ–‡ãŒè¡¨ç¤ºã•ã‚Œã¾ã™"
        )

        if selected_display:
            # é¸æŠã•ã‚ŒãŸå˜èªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            selected_index = word_options.index(selected_display)
            word_info = sorted_df.iloc[selected_index]
            selected_word = word_info['Word']

            st.markdown("---")
            st.markdown(f"Example sentences for: **{selected_word}**")
            section = word_info['Section']
            no = word_info['No.']
            status = word_info['Status']
            status_emoji = get_status_emoji(status)
            info_text = f"**Section:** {section} | **No.:** {no}"
            info_text += f" | **Status:** {status} {status_emoji}"
            st.markdown(info_text)

            # ä¾‹æ–‡ã‚’è¡¨ç¤ºï¼ˆä¿å­˜ã•ã‚ŒãŸIDã‚’ä½¿ç”¨ï¼‰
            try:
                sentence_ids = word_info.get('sentence_ids', [])
                all_sentences = get_sentence_texts(sentence_ids)

                if all_sentences:
                    for i, sentence in enumerate(all_sentences, 1):
                        st.subheader(sentence)
                else:
                    st.info("ã“ã®å˜èªã«ã¯ä¾‹æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            except Exception as e:
                st.error(f"ä¾‹æ–‡ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        st.info("æœªç¿’å¾—å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


if __name__ == "__main__":
    main()
